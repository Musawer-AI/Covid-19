{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Em_CnQ8nJA"
      },
      "source": [
        "# Predicting COVID-19 patient's death situation.\n",
        "This notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting (given a Covid-19 patient's current symptom, status, and medical history) whether the patient is in high risk or not..\n",
        "\n",
        "\n",
        "We're going to take the following approach:\n",
        "1. Problem Definition\n",
        "2. Data\n",
        "3. Evaluation\n",
        "4. Features\n",
        "5. Modelling\n",
        "6. Experimentation\n",
        "\n",
        "## 1. Problem Definition\n",
        "In a statemant\n",
        "> Given clinical parameters about a patient, can we predict whether or not they are in high risk or not?..\n",
        "\n",
        "## 2. Data\n",
        "> The original data came from Kaggle. https://www.kaggle.com/datasets/meirnizri/covid19-dataset\n",
        "\n",
        "## 3. Evaluation\n",
        "> If we can reach to or morethan 90% accuracy at predicting whether or not they are in high risk or not during the proof of concept, would be nice.\n",
        "\n",
        "## 4. Features\n",
        "* sex: 1 for female and 2 for male.\n",
        "* age: of the patient.\n",
        "* classification: covid test findings. \n",
        "  * Values \n",
        "    * 1-3 mean that the patient was diagnosed with covid in different degrees. \n",
        "    * 4 or higher means that the patient is not a carrier of covid or that the   test is inconclusive.\n",
        "* patient type: type of care the patient received in the unit. \n",
        "  * 1 for returned home \n",
        "  * 2 for hospitalization.\n",
        "* pneumonia: whether the patient already have air sacs inflammation or not.\n",
        "* pregnancy: whether the patient is pregnant or not.\n",
        "* diabetes: whether the patient has diabetes or not.\n",
        "* copd: Indicates whether the patient has Chronic obstructive pulmonary disease or not.\n",
        "* asthma: whether the patient has asthma or not.\n",
        "* inmsupr: whether the patient is immunosuppressed or not.\n",
        "* hypertension: whether the patient has hypertension or not.\n",
        "* cardiovascular: whether the patient has heart or blood vessels related disease.\n",
        "* renal chronic: whether the patient has chronic renal disease or not.\n",
        "* other disease: whether the patient has other disease or not.\n",
        "* obesity: whether the patient is obese or not.\n",
        "* tobacco: whether the patient is a tobacco user.\n",
        "* usmr: Indicates whether the patient treated medical units of the first, second or third level.\n",
        "* medical unit: type of institution of the National Health System that provided the care.\n",
        "* intubed: whether the patient was connected to the ventilator.\n",
        "* icu: Indicates whether the patient had been admitted to an Intensive Care Unit.\n",
        "* date died: If the patient died indicate the date of death, and 9999-99-99 otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJZZKH_FAV9N"
      },
      "source": [
        "## Preparing the tools\n",
        "We're going to use pandas, numpy and matplotlib for data analysis and manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP7uZ2iX9Ccc"
      },
      "outputs": [],
      "source": [
        "# Regular EDA (exploratory data analysis) and plotting libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"seaborn\")\n",
        "import seaborn as sns\n",
        "\n",
        "# Models from sklearn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "uJyQPEPF0ygD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"drive/MyDrive/Data/Covid Data.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "od1guunIyvCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration (exploratory data analysis or EDA)\n",
        "\n",
        "The goal here is to find out more about the data and become a subject matter expert on the dataset you working with.\n",
        "\n",
        "1. What question(s) are you tring to solve?\n",
        "2. What kind of data we have and how we treat different types? \n",
        "3. What's missing form data and how do you deal with it? \n",
        "4. Where are the outliers and why should you care about them?\n",
        "5. How can you add, change or remove features to get more out of your data?"
      ],
      "metadata": {
        "id": "Sdsyd8Ws1P8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "-POWDFeS03dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "HRdzN_x22eDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the number of unique values by column\n",
        "for column in df.columns:\n",
        "  print(column,\"=>\\t\",len(df[column].unique()))"
      ],
      "metadata": {
        "id": "_N7tT2eO5sF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "10ysBCc_ARcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "* We have some features that we expect them to have just 2 unique values but we see that these features have 3 or 4 unique values. For example the feature \"PNEUMONIA\" has 3 unique values (1,2,99) 99 represents NaN values. Hence we will just take the rows that includes 1 and 2 values (values as 97 and 99 are missing data).\n",
        "\n",
        "* In \"DATE_DIED\" column, we have 971633 \"9999-99-99\" values which represent alive patients so i will take this feature as a \"DEATH\" that includes wether the patient died or not"
      ],
      "metadata": {
        "id": "2GV-tAia6Vwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of the orignal dataframe\n",
        "df_tmp  = df.copy()"
      ],
      "metadata": {
        "id": "2CI-wJ7jGLu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the columns with 99 and 97 values\n",
        "for column in df_tmp.columns:\n",
        "  if 99 in df_tmp[column].value_counts() or 97 in df_tmp[column].value_counts() or 98 in df_tmp[column].value_counts():\n",
        "    print(column)"
      ],
      "metadata": {
        "id": "FYiZGhVd6uSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp = df_tmp[(df_tmp.PNEUMONIA == 1) | (df_tmp.PNEUMONIA == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.DIABETES == 1) | (df_tmp.DIABETES == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.COPD == 1) | (df_tmp.COPD == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.ASTHMA == 1) | (df_tmp.ASTHMA == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.INMSUPR == 1) | (df_tmp.INMSUPR == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.HIPERTENSION == 1) | (df_tmp.HIPERTENSION == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.OTHER_DISEASE == 1) | (df_tmp.OTHER_DISEASE == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.CARDIOVASCULAR == 1) | (df_tmp.CARDIOVASCULAR == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.OBESITY == 1) | (df_tmp.OBESITY == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.RENAL_CHRONIC == 1) | (df_tmp.RENAL_CHRONIC == 2)]\n",
        "df_tmp = df_tmp[(df_tmp.TOBACCO == 1) | (df_tmp.TOBACCO == 2)]"
      ],
      "metadata": {
        "id": "mZRpiRyI5lYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If we plot PREGNANT column, We see that all \"97\" values are for males and males can not be pregnant so we will convert 97 to 2."
      ],
      "metadata": {
        "id": "ISbKKmn2-WjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(df_tmp.PREGNANT, df_tmp.SEX).plot(kind=\"bar\", \n",
        "                                              figsize=(10, 6),\n",
        "                                              cmap=\"tab10\");\n",
        "plt.title(\"Pregnant according to Sex\")\n",
        "plt.xlabel(\"1 = Pregnant, 2 = Not pregnant, 97/98 = Missing\")\n",
        "plt.legend([\"Female\", \"Male\"])\n",
        "plt.xticks(rotation=0);"
      ],
      "metadata": {
        "id": "gNSY1lE8_iod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting 97 to 2\n",
        "df_tmp.PREGNANT = df_tmp.PREGNANT.replace(97, 2)\n",
        "\n",
        "# Getting rid of missing values (98)\n",
        "df_tmp = df_tmp[(df_tmp.PREGNANT==1) | (df_tmp.PREGNANT==2)]\n",
        "df_tmp.PREGNANT.value_counts()"
      ],
      "metadata": {
        "id": "Em_cCs1e-VXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the label from date-column\n",
        "df_tmp[\"DEATH\"] = [2 if date==\"9999-99-99\" else 1 for date in df_tmp.DATE_DIED]\n",
        "df_tmp.DEATH.value_counts()"
      ],
      "metadata": {
        "id": "R18vQZ83HfsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In \"INTUBED\" and \"ICU\" features there are too many missing values so i will drop them. Also we don't need \"DATE_DIED\" column anymore because we used this feature as a \"DEATH\" feature.\n"
      ],
      "metadata": {
        "id": "sH47w800HYVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.drop(columns=[\"INTUBED\",\"ICU\",\"DATE_DIED\"], inplace=True)"
      ],
      "metadata": {
        "id": "QKDPyT5uHQGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the number of unique values by column\n",
        "for column in df_tmp.columns:\n",
        "  print(column,\"=>\\t\",len(df_tmp[column].unique()))"
      ],
      "metadata": {
        "id": "RS_XAgRZ2gIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization"
      ],
      "metadata": {
        "id": "kMrac5bsIcaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(df_tmp.DEATH, df_tmp.SEX).plot(kind=\"bar\", \n",
        "                                                       figsize=(10, 6), \n",
        "                                                       cmap=\"tab10\")\n",
        "plt.title(\"Death Frequency according to Sex\")\n",
        "plt.xlabel(\"1 = Death, 2 = Alive\")\n",
        "plt.ylabel(\"Amount\")\n",
        "plt.legend([\"Female\", \"Male\"])\n",
        "plt.xticks(rotation=0);"
      ],
      "metadata": {
        "id": "GsREcWmXIOvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.AGE.plot(kind=\"hist\",\n",
        "            figsize=(10, 6),\n",
        "            cmap=\"tab10\");"
      ],
      "metadata": {
        "id": "HVcw7YPCOdqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(df_tmp.DEATH, df_tmp.PATIENT_TYPE).plot(kind=\"bar\", \n",
        "                                                       figsize=(10, 6), \n",
        "                                                       cmap=\"tab10\")\n",
        "plt.title(\"Death Frequency according to Patient type\")\n",
        "plt.xlabel(\"1 = Death, 2 = Alive\")\n",
        "plt.ylabel(\"Amount\")\n",
        "plt.legend([\"returned home\", \"hospitalization\"])\n",
        "plt.xticks(rotation=0);"
      ],
      "metadata": {
        "id": "OA9zM-CfPlJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matix = df_tmp.corr()\n",
        "fig, ax = plt.subplots(figsize=(20, 12))\n",
        "ax = sns.heatmap(corr_matix, \n",
        "                 annot=True, \n",
        "                 linewidths=0.5, \n",
        "                 fmt=\".2f\", \n",
        "                 cmap=\"YlGnBu\")"
      ],
      "metadata": {
        "id": "504sYDS6kNe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the features that have low correlation with \"DEATH\" feature.\n",
        "unrelevant_columns = [\"SEX\",\"PREGNANT\",\"COPD\",\"ASTHMA\",\"INMSUPR\",\"OTHER_DISEASE\",\"CARDIOVASCULAR\",\n",
        "                      \"OBESITY\",\"TOBACCO\"]\n",
        "\n",
        "df_tmp.drop(columns=unrelevant_columns,inplace=True)\n",
        "df_tmp.info()"
      ],
      "metadata": {
        "id": "e5BGWAjCkt3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "* We got well accuracy with Logistic Regression.\n",
        "* But it can mislead us so we have to check the other metrics.\n",
        "* When we look at the F1 Score it says that we predicted the patients who survived well but we can't say the same thing for dead patients.\n",
        "* Also we see the same thing when we check the confusion matrix. This problem is based on imbalance dataset as i mentioned about it."
      ],
      "metadata": {
        "id": "ef1-yW4xt8l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(x=df_tmp.DEATH,\n",
        "                   palette=sns.color_palette(\"YlGnBu\"))\n",
        "plt.bar_label(ax.containers[0])\n",
        "plt.title(\"Death Distribution\");"
      ],
      "metadata": {
        "id": "zYOYFaLTt7o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How To Solve Imbalance Dataset Problem\n",
        "* Loading More Datas\n",
        "* Changing The Perfomance Metrics\n",
        "* Resampling (Undersampling or Oversampling)\n",
        "* Changing The Algorithm\n",
        "* Penalized Models etc.\n",
        "\n",
        "we're going to use Undersampling for this case because we already have too many patients.\n",
        "* Undersampling : Undersampling is a technique to balance uneven datasets by keeping all of the data in the minority class and decreasing the size of the majority class.\n",
        "* If we use Oversampling our row number will increase so this is too many rows for computer.\n",
        "* If i can't solve the problem with Undersampling i will use the others"
      ],
      "metadata": {
        "id": "IDNE2_0cuR-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# split data in to x & y\n",
        "x, y = df_tmp.drop(\"DEATH\", axis=1), df_tmp[\"DEATH\"]\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "x_resampled, y_resampled = rus.fit_resample(x, y)"
      ],
      "metadata": {
        "id": "yoXQGcYMunmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sns.countplot(x=y_resampled,\n",
        "                   palette=sns.color_palette(\"YlGnBu\"))\n",
        "plt.bar_label(ax.containers[0])\n",
        "plt.title(\"Death Distribution\");"
      ],
      "metadata": {
        "id": "7K07F4XpuvVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Modlling"
      ],
      "metadata": {
        "id": "pDpJpvrakCyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a fuction to split the data into train, validation and test sets\n",
        "def train_val_test_split(X, \n",
        "                          y,\n",
        "                          frac_train=0.6, \n",
        "                          frac_val=0.15, \n",
        "                          frac_test=0.25,\n",
        "                          random_state=42):\n",
        "    '''\n",
        "    Splits a Pandas dataframe into three subsets (train, val, and test)\n",
        "    following fractional ratios provided by the user, where each subset is\n",
        "    stratified by the values in a specific column (that is, each subset has\n",
        "    the same relative frequency of the values in the column). It performs this\n",
        "    splitting by running train_test_split() twice.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : Features\n",
        "    y : Target\n",
        "    frac_train : float\n",
        "    frac_val   : float\n",
        "    frac_test  : float\n",
        "        The ratios with which the dataframe will be split into train, val, and\n",
        "        test data. The values should be expressed as float fractions and should\n",
        "        sum to 1.0.\n",
        "    random_state : int, None, or RandomStateInstance\n",
        "        Value to be passed to train_test_split().\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test :\n",
        "        Dataframes containing the three splits.\n",
        "    '''\n",
        "\n",
        "    if frac_train + frac_val + frac_test != 1.0:\n",
        "        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n",
        "                         (frac_train, frac_val, frac_test))\n",
        "\n",
        "    # Split original dataframe into train and temp dataframes.\n",
        "    X_train, df_temp, y_train, y_temp = train_test_split(X,\n",
        "                                                          y,\n",
        "                                                          stratify=y,\n",
        "                                                          test_size=(1.0 - frac_train),\n",
        "                                                          random_state=random_state)\n",
        "\n",
        "    # Split the temp dataframe into val and test dataframes.\n",
        "    relative_frac_test = frac_test / (frac_val + frac_test)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(df_temp,\n",
        "                                                      y_temp,\n",
        "                                                      test_size=relative_frac_test,\n",
        "                                                      random_state=random_state)\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "metadata": {
        "id": "s5XMHfYjkAzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_resampled)"
      ],
      "metadata": {
        "id": "W19il5FWOlpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting the data into train, validation and test\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X=x_resampled, y=y_resampled)\n",
        "\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "w-oWIDbLIbUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now've got our data split into training, validation and test sets, it time to build a machine learning model.\n",
        "\n",
        "We'll train it (find the patterns) on the training sets.\n",
        "\n",
        "And we'll test it (use the patterns) on the test set.\n",
        "\n",
        "We'er going to try 3 different machine learning models:\n",
        "1. Logistic Regression\n",
        "2. K-Nearest Neighbours classifier\n",
        "3. RandomForestClassifier\n",
        "4. Support Vector Machine"
      ],
      "metadata": {
        "id": "bDKStzk4llys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
        "    \"SVC\": SVC(kernal=\"rbf\")\n",
        "}\n",
        "\n",
        "def fit_and_score(models, x_train, x_test, y_train, y_test):\n",
        "  \"\"\"\n",
        "  Fits and evaluates given machine learning models.\n",
        "  models : a dict of different Scikit-learn machine learning models\n",
        "  x_train : training data (no labels)\n",
        "  x_test : testing data (no labels)\n",
        "  y_train : training labels\n",
        "  y_test : test labels\n",
        "  \"\"\"\n",
        "\n",
        "  # Setup random seed\n",
        "  np.random.seed(42)\n",
        "\n",
        "  # Make a dictionary to keep models score\n",
        "  model_scores = {}\n",
        "\n",
        "  # Loop through models\n",
        "  for model_name, model in models.items():\n",
        "    # Fit the model to the data\n",
        "    clf = model.fit(x_train, y_train)\n",
        "  \n",
        "    # Evaluate the model and append its score to model_score\n",
        "    model_scores[model_name] = clf.score(x_test, y_test)\n",
        "\n",
        "  return model_scores"
      ],
      "metadata": {
        "id": "MDBqmYOnletz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_score = fit_and_score(models, X_train, X_test, y_train, y_test)\n",
        "models_score"
      ],
      "metadata": {
        "id": "80Fk-vZRyZ3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Comparison"
      ],
      "metadata": {
        "id": "Li-Z8K9t0IBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_compare = pd.DataFrame(models_score, index=[\"accuracy\"])\n",
        "\n",
        "model_compare.T.plot(kind=\"bar\",\n",
        "                     figsize=(10, 6),\n",
        "                     cmap=\"tab10\")\n",
        "\n",
        "plt.xticks(rotation=0);"
      ],
      "metadata": {
        "id": "JtiVhBIXyagR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As we can see the SVC model not prefromed good compare to other models so we eliminate the SVC Model"
      ],
      "metadata": {
        "id": "Jdk8B5hfc3Od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now've got our baseline models... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n",
        "\n",
        "Let's look at the following:\n",
        "* Hyperparameter tuning\n",
        "* Feature importance\n",
        "* Confusion matrix \n",
        "* Cross-validation\n",
        "* Precision \n",
        "* Recall\n",
        "* F1 score\n",
        "* Classification report\n",
        "* ROC curve\n",
        "* Area under the curve (AUC)\n",
        "\n",
        "## Hyperparameter tuning with RandomizedSearchCV\n",
        "\n",
        "We're going to tune:\n",
        "* LogisticRegression()\n",
        "* RandomForestClassifier()\n",
        "\n",
        "... using RandomizedSearchCV"
      ],
      "metadata": {
        "id": "vE4AuVxk09Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a hyperparameter grid for LogisticRegression\n",
        "log_reg_grid = {\n",
        "    'penalty' : ['l2'],\n",
        "    'C' : np.logspace(-4, 4, 20),\n",
        "    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n",
        "}\n",
        "\n",
        "# Create a hyperparameter grid for RandomForestClassifier\n",
        "rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n",
        "           \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "           \"max_depth\": [3, 5, 10],\n",
        "           \"min_samples_split\": np.arange(2, 20, 2),\n",
        "           \"min_samples_leaf\": np.arange(1, 20, 2)}\n",
        "\n",
        "# Create a hyperparameter grid for KNN\n",
        "knn_grid = {'n_neighbors' : np.arange(1, 30, 1),\n",
        "            'leaf_size': np.arange(1, 50, 1),\n",
        "            'weights' : ['uniform','distance'],\n",
        "            'p': [1, 2],\n",
        "            'metric' : ['minkowski','euclidean','manhattan']}"
      ],
      "metadata": {
        "id": "VZL8htbT0kth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got hyperparameter grids setup for each of our models, \n",
        "let's tune them using RandomizedSearchCV."
      ],
      "metadata": {
        "id": "TLgCGWfy3RiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_resampled.columns"
      ],
      "metadata": {
        "id": "OB80Ef5c9ZxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup randomizedSearchCv for LogisticRegression\n",
        "rs_log_reg = RandomizedSearchCV(LogisticRegression(max_iter=5000),\n",
        "                                param_distributions=log_reg_grid,\n",
        "                                n_iter=100,\n",
        "                                cv=5,\n",
        "                                verbose=1)\n",
        "\n",
        "# Fit random hyperparameter search model for LogisticRegression\n",
        "rs_log_reg.fit(X_val, y_val)"
      ],
      "metadata": {
        "id": "JBrGRR4zwtju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_log_reg.best_params_"
      ],
      "metadata": {
        "id": "1Hp-40Z2mK3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_log_score = rs_log_reg.score(X_test_normal, y_test)\n",
        "rs_log_score"
      ],
      "metadata": {
        "id": "8lcxmnvt9x4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup randomizedSearchCv for LogisticRegression\n",
        "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
        "                            param_distributions=rf_grid,\n",
        "                            n_iter=100,\n",
        "                            cv=5,\n",
        "                            verbose=1)\n",
        "\n",
        "# Fit random hyperparameter search model for LogisticRegression\n",
        "rs_rf.fit(X_val, y_val)"
      ],
      "metadata": {
        "id": "HetYt9Dgj1eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_rf.best_params_"
      ],
      "metadata": {
        "id": "WJjY3UsBjsev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_score = rs_rf.score(X_test, y_test)\n",
        "rf_score"
      ],
      "metadata": {
        "id": "TED1YxTsjwz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup randomizedSearchCv for LogisticRegression\n",
        "rs_knn = RandomizedSearchCV(KNeighborsClassifier(),\n",
        "                                param_distributions=knn_grid,\n",
        "                                n_iter=100,\n",
        "                                cv=5,\n",
        "                                verbose=True)\n",
        "\n",
        "# Fit random hyperparameter search model for LogisticRegression\n",
        "rs_knn.fit(X_val, y_val)"
      ],
      "metadata": {
        "id": "93DuvvI2Q-fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs_knn.best_params_"
      ],
      "metadata": {
        "id": "Xm8OX-EyRVBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_score = rs_knn.score(X_test, y_test)\n",
        "knn_score"
      ],
      "metadata": {
        "id": "asZf-2iIRZbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* compare the tunned models"
      ],
      "metadata": {
        "id": "H9Oo0efmxSPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_score = {\n",
        " 'Logistic Regression': rs_log_score,\n",
        " 'KNN': knn_score,\n",
        " 'RandomForestClassifier': rf_score,\n",
        "}\n",
        "\n",
        "model_compare = pd.DataFrame(model_score, index=[\"accuracy\"])\n",
        "model_compare.T.plot(kind=\"bar\",\n",
        "                     figsize=(10, 6),\n",
        "                     cmap=\"tab10\")\n",
        "\n",
        "plt.xticks(rotation=0);"
      ],
      "metadata": {
        "id": "XBOcpLWzRbz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning with GridSearchCV\n",
        "\n",
        "Since our RandomForestClassifier model provides the best scores so far, we'll try and improve them again using GridSearchCV..."
      ],
      "metadata": {
        "id": "R7_D0FulyfKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a hyperparameter grid for RandomForestClassifier\n",
        "rf_grid = {'bootstrap': [True],\n",
        "            'max_depth': [80, 90, 100, 110],\n",
        "            'max_features': [2, 3],\n",
        "            'min_samples_leaf': [3, 4, 5],\n",
        "            'min_samples_split': [8, 10, 12],\n",
        "            'n_estimators': [100, 200, 300, 1000]}\n",
        "\n",
        "# Setup grid hyperparameter search for RandomForestClassifier\n",
        "gs_rf = GridSearchCV(estimator=RandomForestClassifier(),\n",
        "                     param_grid=rf_grid,\n",
        "                     cv=5,\n",
        "                     verbose=True)\n",
        "\n",
        "# fit grid hyperparameter searcg model for RandomForestClassifier\n",
        "gs_rf.fit(X_val, y_val)"
      ],
      "metadata": {
        "id": "4ZLCBjk2yqUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_rf.best_params_"
      ],
      "metadata": {
        "id": "5CfDowIDz3_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gs_rf.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "DDuduuc643K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluting our tuned machine learning classifier, beyond accuracy\n",
        "\n",
        "* ROC curve and AUC Score\n",
        "* confusion Matrix\n",
        "* Classification Report\n",
        "* Precision\n",
        "* Recall\n",
        "* F1 Score\n",
        "\n",
        "... and it would be great if cross-validation was used where possible.\n",
        "\n",
        "To make comparisons and evaluate our trained model, first we need to make predictions."
      ],
      "metadata": {
        "id": "kq_-z7efo4Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a model with tuned model hyperparameters\n",
        "clf = RandomForestClassifier(max_depth= 90,\n",
        "                               max_features= 3,\n",
        "                               min_samples_leaf= 4,\n",
        "                               min_samples_split= 10,\n",
        "                               n_estimators= 100)\n",
        "\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "XuXY3sgY45ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with new model\n",
        "y_preds = clf.predict(X_test)\n",
        "y_preds[:10]"
      ],
      "metadata": {
        "id": "uWoBt7jMpi6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC Curve and calculate AUC metrics\n",
        "RocCurveDisplay.from_estimator(estimator=clf, X=X_test, y=y_test);"
      ],
      "metadata": {
        "id": "rRPh5gqAp1O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion Matrix\n",
        "print(confusion_matrix(y_true=y_test, y_pred=y_preds))"
      ],
      "metadata": {
        "id": "380B08qZqsyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X_test, y=y_test);"
      ],
      "metadata": {
        "id": "uWFkMKzQrh6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now we've got a ROC curve, and AUC metrics and a confusion matrix, let's get a classification report as well as cross-validated precision, recall and f1-score"
      ],
      "metadata": {
        "id": "b7HJqFAqsOh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true=y_test, y_pred=y_preds))"
      ],
      "metadata": {
        "id": "glrBQhCZsFtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate evaluation metrics using cross-validation\n",
        "\n",
        "We're going to calculate precision, recall and f1-score of our model using cross-validation and to do so we'll using `cross_val_score()` "
      ],
      "metadata": {
        "id": "KAud2MOwsX0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate accuracy\n",
        "cv_acc = cross_val_score(clf,\n",
        "                         cv=5,\n",
        "                         X=x_resampled, \n",
        "                         y=y_resampled,\n",
        "                         scoring=\"accuracy\")\n",
        "cv_acc"
      ],
      "metadata": {
        "id": "FcvtQDCusllQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate precision\n",
        "cv_precision = cross_val_score(clf,\n",
        "                         cv=5,\n",
        "                         X=x_resampled, \n",
        "                         y=y_resampled,\n",
        "                         scoring=\"precision\")\n",
        "cv_precision"
      ],
      "metadata": {
        "id": "P9oGz6dgs57X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate recall\n",
        "cv_recall = cross_val_score(clf,\n",
        "                            cv=5,\n",
        "                            X=x_resampled, \n",
        "                            y=y_resampled,\n",
        "                            scoring=\"recall\")\n",
        "cv_recall"
      ],
      "metadata": {
        "id": "IGDOUw0YtYn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validate f1\n",
        "cv_f1 = cross_val_score(clf,\n",
        "                        cv=5,\n",
        "                        X=x_resampled, \n",
        "                        y=y_resampled,\n",
        "                        scoring=\"f1\")\n",
        "cv_f1"
      ],
      "metadata": {
        "id": "qMcLkyh4trq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize cross-validated metrics\n",
        "cv_metrics = pd.DataFrame({\"Accuracy\": np.mean(cv_acc),\n",
        "                           \"Percision\": np.mean(cv_precision),\n",
        "                           \"Recall\": np.mean(cv_recall),\n",
        "                           \"F1\": np.mean(cv_f1)},\n",
        "                           index=[0])\n",
        "cv_metrics"
      ],
      "metadata": {
        "id": "cW3Nl1hyt1vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_metrics.T.plot.bar(title=\"Corss-validated classification metrics\",\n",
        "                      legend=False,\n",
        "                      cmap=\"tab10\");"
      ],
      "metadata": {
        "id": "nQSGO5uht-Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance \n",
        "\n",
        "Feature Importance is another way as asking, \"Which features contributed most to the outcomes of the mdoel and how did they contribute?\"\n",
        "\n",
        "Finding feature importance is different for each machine learning model. One way to find feature importance is to search for \" (MODEL NAME) feature importance\". "
      ],
      "metadata": {
        "id": "DxekAt7UuaiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "sKRpDpNXuBDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the coefficients\n",
        "clf.feature_importances_"
      ],
      "metadata": {
        "id": "QRPfQ-Dluh20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Match feature_importances_ values to columns\n",
        "feature_dic = dict(zip(X_train.columns, list(clf.feature_importances_)))\n",
        "feature_dic"
      ],
      "metadata": {
        "id": "9SepOXVhuqny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize feature importance\n",
        "feature_df = pd.DataFrame(feature_dic, index=[0])\n",
        "feature_df.T.plot.bar(title=\"Feature Importance\", legend=False);"
      ],
      "metadata": {
        "id": "FsPsktmBxSDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}